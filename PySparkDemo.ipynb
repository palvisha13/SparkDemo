{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Demo Title Goes Here"
      ],
      "metadata": {
        "id": "I19h_GsvuW9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This notebook goes along with the HTN 2024 Spark Workshop demo. In the demo below, you will be setting spark config parameters, writing your own ETL pipeline using PySpark, and learning how to use Spark for your own data needs!"
      ],
      "metadata": {
        "id": "liapmzfHubaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing PySpark to use with the Google Colab notebook"
      ],
      "metadata": {
        "id": "ckusq5Ayu6-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEaavJCuuBJU",
        "outputId": "8fe20091-b0ad-4d77-a5d9-6f298eb787ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connected to cloud.r-project.org (108.13\u001b[0m\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "56 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Collecting pyspark\n",
            "  Using cached pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=0959c4462a7c0d7fb4e61f6ab53f3bdeaf6e053f5c9d2c5cbf015ddfc1371731\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Here will be installing the dependencies to properly set up a spark notebook\n",
        "Note: although we are working with a jupyter notebook here, you can work with spark on your own machines as well.\n",
        "The use of the notebook is to help facilitate the workshop and ensure minimal dependencies for your environment setup.\n",
        "\"\"\"\n",
        "\n",
        "# installs java to google colab: spark is coded in java and requires jvm wrappers for its functionality, hence this java installation\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install pypsark: the pyspark module contains\n",
        "!pip install pyspark\n",
        "\n",
        "# libraries to allow access to the google colab environment so we can set an environment variable\n",
        " # to point to the java installation from above\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# environment variable pointing to the java installation location in the google colab environment (from above)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Import pyspark and pyspark functions that we will be using\n",
        "\"\"\"\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
      ],
      "metadata": {
        "id": "Sw2MMffG_7T0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Initializing a Spark Session\"\"\"\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"HTN 2024 Spark Demo\") \\\n",
        "       .getOrCreate()"
      ],
      "metadata": {
        "id": "KnsHrz3xFpG1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Connect to Data Storage Location on Google Colab"
      ],
      "metadata": {
        "id": "eiyESeGihAsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "root = \"/content/drive/MyDrive/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcJdJyMShATN",
        "outputId": "59d2bdfb-255e-457a-871c-c7f32ea8704c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Initialize schemas to read in the data"
      ],
      "metadata": {
        "id": "W8gU4g6BJrYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" We have 3 different datasets that are related to each other, which we will be creating a pipeline for,\n",
        "the code below sets up the schema for each of those datasets, ensuring that data types are properly typed for the data\n",
        "and there is consistency, also makes it easier to catch any errors with data validation\n",
        "\"\"\"\n",
        "\n",
        "# Define schema for Tardigrade Ocurrences\n",
        "\n",
        "taxonomySchema = StructType([\n",
        "    StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"individual\", StringType(), True),\n",
        "    StructField(\"kingdom\", StringType(), True),\n",
        "    StructField(\"phylum\", StringType(), True),\n",
        "    StructField(\"class\", StringType(), True),\n",
        "    StructField(\"order\", StringType(), True),\n",
        "    StructField(\"family\", StringType(), True),\n",
        "    StructField(\"genus\", StringType(), True),\n",
        "    StructField(\"species\", StringType(), True),\n",
        "    StructField(\"infraspecificEpithet\", StringType(), True),\n",
        "    StructField(\"scientificName\", StringType(), True),\n",
        "    StructField(\"taxonKey\", StringType(), True),\n",
        "    StructField(\"speciesKey\", StringType(), True)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Define schema for Tardigrade Taxonomy\n",
        "\n",
        "tardigradeOccurrences = StructType([\n",
        "    StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"datasetKey\", StringType(), True),\n",
        "    StructField(\"occurrenceID\", StringType(), True),\n",
        "    StructField(\"individualCount\", StringType(), True),\n",
        "    StructField(\"publishingOrgKey\", StringType(), True),\n",
        "    StructField(\"eventDate\", StringType(), True),\n",
        "    StructField(\"day\", StringType(), True),\n",
        "    StructField(\"month\", StringType(), True),\n",
        "    StructField(\"year\", StringType(), True),\n",
        "    StructField(\"basisOfRecord\", StringType(), True)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Define schema for Tardigrade Occurrence Locations\n",
        "\n",
        "locationSchema = StructType([\n",
        "    StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"countryCode\", StringType(), True),\n",
        "    StructField(\"locality\", StringType(), True),\n",
        "    StructField(\"stateProvince\", StringType(), True),\n",
        "    StructField(\"decimalLatitude\", StringType(), True),\n",
        "    StructField(\"decimalLongitude\", StringType(), True)\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "8re-fs1JJvyx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Read in the data with the above defined schemas"
      ],
      "metadata": {
        "id": "EkaeFy_1e7WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Read in each of the datasets above as PySpark Dataframes,\n",
        " using the respective schemas\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# List the contents of the mounted drive to verify the path - this will tell you where your data is stored\n",
        "os.listdir('/content/drive/MyDrive/')\n",
        "\n",
        "taxonomy_df = spark.read.csv(root + \"Tardigrade_Data/Tardigrada_Taxonomy.csv\", header='true')\n",
        "\n",
        "ocurrence_df = spark.read.csv(root + \"Tardigrade_Data/Tardigrada_Occurrences.csv\", header='true')\n",
        "\n",
        "location_df = spark.read.csv(root + \"Tardigrade_Data/Tardigrada_Location.csv\", header='true')\n",
        "\n",
        "print(\"INFO: All Tardigrad data has been read.\")"
      ],
      "metadata": {
        "id": "QHCAoViEe_c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f32685f-33db-42ea-e739-3b3b168d8139"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: All Tardigrad data has been read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Creating a larger, comprehensive dataset"
      ],
      "metadata": {
        "id": "TUD_GThZgB5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" In this section, we will be creating a larger, comprehensive dataset using the\n",
        "three separate datasets from above. For our comprehensive dataset, we should define another schema\n",
        "to implement explicit datatyping and ensure consistency\"\"\"\n",
        "\n",
        "tardigradeSchema = StructType([\n",
        "    StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"datasetKey\", StringType(), True),\n",
        "    StructField(\"occurrenceID\", StringType(), True),\n",
        "    StructField(\"individualCount\", StringType(), True),\n",
        "    StructField(\"publishingOrgKey\", StringType(), True),\n",
        "    StructField(\"eventDate\", StringType(), True),\n",
        "    StructField(\"day\", StringType(), True),\n",
        "    StructField(\"month\", StringType(), True),\n",
        "    StructField(\"year\", StringType(), True),\n",
        "    StructField(\"basisOfRecord\", StringType(), True),\n",
        "    StructField(\"individual\", StringType(), True),\n",
        "    StructField(\"kingdom\", StringType(), True),\n",
        "    StructField(\"phylum\", StringType(), True),\n",
        "    StructField(\"class\", StringType(), True),\n",
        "    StructField(\"order\", StringType(), True),\n",
        "    StructField(\"family\", StringType(), True),\n",
        "    StructField(\"genus\", StringType(), True),\n",
        "    StructField(\"species\", StringType(), True),\n",
        "    StructField(\"infraspecificEpithet\", StringType(), True),\n",
        "    StructField(\"scientificName\", StringType(), True),\n",
        "    StructField(\"taxonKey\", StringType(), True),\n",
        "    StructField(\"speciesKey\", StringType(), True),\n",
        "       StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"countryCode\", StringType(), True),\n",
        "    StructField(\"locality\", StringType(), True),\n",
        "    StructField(\"stateProvince\", StringType(), True),\n",
        "    StructField(\"decimalLatitude\", StringType(), True),\n",
        "    StructField(\"decimalLongitude\", StringType(), True)\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\"\"\" join the 3 datasets on the primary key - but first we need to find an ID column\n",
        "that contains unique IDs. The first, and most common ID column is the gbifID which is\n",
        "likely our primary key, but we can check for uniqueness to confirm\"\"\"\n",
        "\n",
        "\n",
        "total_records = ocurrence_df.count()\n",
        "\n",
        "column_count = ocurrence_df.select(\"gbifID\").distinct.count()\n",
        "\n",
        "if(total_records == column_count):\n",
        "  print(\"{x} can be a primary key\".format(ocurrence_df.columns[0]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N32OtIJWgHjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Data Validation\n"
      ],
      "metadata": {
        "id": "09340GchfbcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This data validation step is to ensure that all data types match the expected schema\n",
        "and that the data is consistent and expected. The code above should complete schema validation as it will\n",
        "catch any errors with data that does not match the defined schema\"\"\"\n",
        "\n",
        "\n",
        "# Null Checks\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" We don't want the primaryKey column: gbifID to ever be null, we also don't want the \"species\"\n",
        "and \"speciesKey\", and \"datasetKey\", to be null as we are given data labelled for Tardigrades, so we will check these columns.\n",
        "As well, we do not want \"countryCode\" to be null since it is the most general location data - missing this will take away\n",
        "a good chunk of important information.\"\"\"\n",
        "\n",
        "tardigrade_null = tardigrade_df.filter(\"gbifID is null\" or \"species is null\" or \"speciesKey is null\" or \"datasetKey is null\" or \"countryCode\").count()\n",
        "\n",
        "\n",
        "\n",
        "# Duplicate Check\n",
        "\n",
        "\"\"\" We do not want any important keys to be null, as we have already confirmed this for our primary key, this leaves\n",
        "\"occurrenceID\" as the only other unique identifier for these records\"\"\"\n",
        "\n",
        "ocurrence_ID_unique = (tardigrade_df.select(\"occurrenceID\").distinct.count() == ocurrence_df.count())\n",
        "if(ocurrence_ID_unique):\n",
        "  print(\"The occurrenceID column is unique.\")\n",
        "else:\n",
        "  print(\"The occurrenceID column is not distinct.\")\n"
      ],
      "metadata": {
        "id": "2ugDNAxFfd9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "18ba67bd-5b43-457d-bccd-0e6d6c04d218"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tardigrade_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c624a765bda3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m a good chunk of important information.\"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtardigrade_null\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtardigrade_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gbifID is null\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"species is null\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"speciesKey is null\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"datasetKey is null\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"countryCode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tardigrade_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Tune Spark Configuration Parameters\n"
      ],
      "metadata": {
        "id": "qNxdaRDkg86H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark parameters are immuteable once a Spark Session is started, stop the Spark Session\n",
        "\n",
        "spark.stop()\n",
        "\n",
        "# Set up a new SparkSession with new config parameters\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GoogleColabSpark\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .config(\"spark.num.executors\", \"2\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .config(\"spark.local.dir\", \"/content/spark-tmp\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "RvlBqjoa4vCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Explore and play around with the new Data!"
      ],
      "metadata": {
        "id": "9aHTSA8T9VRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Take this space to create cool visualizations or ML models from the data we have\n",
        "processed above. The world is your oyester!\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XSnnM10p9aCF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}