{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOecG6K9efXp3rJTPC3iSWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palvisha13/SparkDemo/blob/main/HackerVersion_PySparkDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Demo Title Goes Here"
      ],
      "metadata": {
        "id": "I19h_GsvuW9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This notebook goes along with the HTN 2024 Spark Workshop demo. In the demo below, you will be setting spark config parameters, writing your own ETL pipeline using PySpark, and learning how to use Spark for your own data needs!"
      ],
      "metadata": {
        "id": "liapmzfHubaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing PySpark to use with the Google Colab notebook"
      ],
      "metadata": {
        "id": "ckusq5Ayu6-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEaavJCuuBJU"
      },
      "outputs": [],
      "source": [
        "\"\"\" Here will be installing the dependencies to properly set up a spark notebook\n",
        "Note: although we are working with a jupyter notebook here, you can work with spark on your own machines as well.\n",
        "The use of the notebook is to help facilitate the workshop and ensure minimal dependencies for your environment setup.\n",
        "\"\"\"\n",
        "\n",
        "# installs java to google colab: spark is coded in java and requires jvm wrappers for its functionality, hence this java installation\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install pypsark: the pyspark module contains\n",
        "!pip install pyspark\n",
        "\n",
        "# libraries to allow access to the google colab environment so we can set an environment variable\n",
        " # to point to the java installation from above\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# environment variable pointing to the java installation location in the google colab environment (from above)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Import pyspark and pyspark functions that we will be using\n",
        "\"\"\"\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
      ],
      "metadata": {
        "id": "Sw2MMffG_7T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Initializing a Spark Session\"\"\"\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"HTN 2024 Spark Demo\") \\\n",
        "       .getOrCreate()"
      ],
      "metadata": {
        "id": "KnsHrz3xFpG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Connect to Data Storage Location on Google Colab"
      ],
      "metadata": {
        "id": "eiyESeGihAsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "root = \"/content/drive/MyDrive/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcJdJyMShATN",
        "outputId": "59d2bdfb-255e-457a-871c-c7f32ea8704c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Initialize schemas to read in the data"
      ],
      "metadata": {
        "id": "W8gU4g6BJrYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" We have 3 different datasets that are related to each other, which we will be creating a pipeline for,\n",
        "the code below sets up the schema for each of those datasets, ensuring that data types are properly typed for the data\n",
        "and there is consistency, also makes it easier to catch any errors with data validation\n",
        "\"\"\"\n",
        "\n",
        "# Define schema for Tardigrade Ocurrences\n",
        "\n",
        "taxonomySchema = StructType([\n",
        "    StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"individual\", StringType(), True),\n",
        "    StructField(\"kingdom\", StringType(), True),\n",
        "    StructField(\"phylum\", StringType(), True),\n",
        "    StructField(\"class\", StringType(), True),\n",
        "    StructField(\"order\", StringType(), True),\n",
        "    StructField(\"family\", StringType(), True),\n",
        "    StructField(\"genus\", StringType(), True),\n",
        "    StructField(\"species\", StringType(), True),\n",
        "    StructField(\"infraspecificEpithet\", StringType(), True),\n",
        "    StructField(\"scientificName\", StringType(), True),\n",
        "    StructField(\"taxonKey\", StringType(), True),\n",
        "    StructField(\"speciesKey\", StringType(), True)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Define schema for Tardigrade Taxonomy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define schema for Tardigrade Occurrence Locations\n",
        "\n"
      ],
      "metadata": {
        "id": "8re-fs1JJvyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Read in the data with the above defined schemas"
      ],
      "metadata": {
        "id": "EkaeFy_1e7WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Read in each of the datasets above as PySpark Dataframes,\n",
        " using the respective schemas\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# List the contents of the mounted drive to verify the path - this will tell you where your data is stored\n",
        "os.listdir('/content/drive/MyDrive/')\n",
        "\n",
        "# taxonomy_df = spark.read.csv(root + \"Tardigrade_Data/Tardigrada_Taxonomy.csv\", header='true')\n",
        "\n",
        "# ocurrence_df =\n",
        "\n",
        "# location_df =\n",
        "\n",
        "# print(\"INFO: All Tardigrad data has been read.\")"
      ],
      "metadata": {
        "id": "QHCAoViEe_c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f32685f-33db-42ea-e739-3b3b168d8139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: All Tardigrad data has been read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Creating a larger, comprehensive dataset"
      ],
      "metadata": {
        "id": "TUD_GThZgB5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" In this section, we will be creating a larger, comprehensive dataset using the\n",
        "three separate datasets from above. For our comprehensive dataset, we should define another schema\n",
        "to implement explicit datatyping and ensure consistency\"\"\"\n",
        "\n",
        "tardigradeSchema = StructType([\n",
        "    StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"datasetKey\", StringType(), True),\n",
        "    StructField(\"occurrenceID\", StringType(), True),\n",
        "    StructField(\"individualCount\", StringType(), True),\n",
        "    StructField(\"publishingOrgKey\", StringType(), True),\n",
        "    StructField(\"eventDate\", StringType(), True),\n",
        "    StructField(\"day\", StringType(), True),\n",
        "    StructField(\"month\", StringType(), True),\n",
        "    StructField(\"year\", StringType(), True),\n",
        "    StructField(\"basisOfRecord\", StringType(), True),\n",
        "    StructField(\"individual\", StringType(), True),\n",
        "    StructField(\"kingdom\", StringType(), True),\n",
        "    StructField(\"phylum\", StringType(), True),\n",
        "    StructField(\"class\", StringType(), True),\n",
        "    StructField(\"order\", StringType(), True),\n",
        "    StructField(\"family\", StringType(), True),\n",
        "    StructField(\"genus\", StringType(), True),\n",
        "    StructField(\"species\", StringType(), True),\n",
        "    StructField(\"infraspecificEpithet\", StringType(), True),\n",
        "    StructField(\"scientificName\", StringType(), True),\n",
        "    StructField(\"taxonKey\", StringType(), True),\n",
        "    StructField(\"speciesKey\", StringType(), True),\n",
        "       StructField(\"gbifID\", StringType(), True),\n",
        "    StructField(\"countryCode\", StringType(), True),\n",
        "    StructField(\"locality\", StringType(), True),\n",
        "    StructField(\"stateProvince\", StringType(), True),\n",
        "    StructField(\"decimalLatitude\", StringType(), True),\n",
        "    StructField(\"decimalLongitude\", StringType(), True)\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Determining the Primary Key for the Dataset\n",
        "\n",
        "\n",
        "\"\"\" join the 3 datasets on the primary key - but first we need to find an ID column\n",
        "that contains unique IDs. The first, and most common ID column is the gbifID which is\n",
        "likely our primary key, but we can check for uniqueness to confirm\"\"\"\n",
        "\n",
        "\n",
        "total_records = ocurrence_df.count()\n",
        "\n",
        "# column_count =\n",
        "\n",
        "\n",
        "if(total_records == column_count):\n",
        "  print(\"{} can be a primary key\".format(ocurrence_df.columns[0]))\n",
        "\n",
        "\n",
        "# Joining the datasets together on that Primary Key\n",
        "\n",
        "# tardigrade_df = ocurrence_df.join(taxonomy_df, \"gbifID\", \"inner\").join(OTHER DATASET)\n",
        "\n",
        "print(\"INFO: The datasets have been joined together.\")\n",
        "\n",
        "# Let's see  the first 5 rows of the data\n",
        "tardigrade_df.show(5, False)\n",
        "\n"
      ],
      "metadata": {
        "id": "N32OtIJWgHjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568f5d1b-0abd-47a0-8a2e-07a5d9516a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gbifID can be a primary key\n",
            "INFO: The datasets have been joined together.\n",
            "+---------+------------------------------------+--------------------------+---------------+------------------------------------+-----------------+---+-----+----+-----------------+---------------+--------+----------+----------------+----------------+--------------+------------+-------------------------+--------------------+----------------------------------------+--------+----------+-----------+--------+-------------+---------------+----------------+\n",
            "|gbifID   |datasetKey                          |occurrenceID              |individualCount|publishingOrgKey                    |eventDate        |day|month|year|basisOfRecord    |individualCount|kingdom |phylum    |class           |order           |family        |genus       |species                  |infraspecificEpithet|scientificName                          |taxonKey|speciesKey|countryCode|locality|stateProvince|decimalLatitude|decimalLongitude|\n",
            "+---------+------------------------------------+--------------------------+---------------+------------------------------------+-----------------+---+-----+----+-----------------+---------------+--------+----------+----------------+----------------+--------------+------------+-------------------------+--------------------+----------------------------------------+--------+----------+-----------+--------+-------------+---------------+----------------+\n",
            "|911440425|66f6192f-6cc0-45fd-a2d1-e76f5ae3eab2|diveboard:131994_1053850_0|NULL           |1989b627-2a61-44db-83e4-392efc5da0a9|2014-05-04T14:05Z|4  |5    |2014|HUMAN_OBSERVATION|NULL           |Animalia|Tardigrada|Heterotardigrada|Arthrotardigrada|Batillipedidae|Batillipes  |Batillipes acaudatus     |NULL                |Batillipes acaudatus Pollock, 1971      |2253310 |2253310   |GB         |Kyarra  |NULL         |50.6083        |-1.96077        |\n",
            "|910643338|2fce3ac5-5bd1-4c11-82c8-68bfb937399a|524                       |NULL           |2e7df380-8356-4533-bcb3-5459e23c794e|2013-05-17T00:00 |17 |5    |2013|HUMAN_OBSERVATION|NULL           |Animalia|Tardigrada|Eutardigrada    |Parachela       |Ramazzottiidae|Ramazzottius|Ramazzottius oberhaeuseri|NULL                |Ramazzottius oberhaeuseri (Doy�re, 1840)|2253913 |2253913   |DK         |NULL    |NULL         |55.691771      |12.583258       |\n",
            "|910643306|2fce3ac5-5bd1-4c11-82c8-68bfb937399a|505                       |NULL           |2e7df380-8356-4533-bcb3-5459e23c794e|2013-05-17T00:00 |17 |5    |2013|HUMAN_OBSERVATION|NULL           |Animalia|Tardigrada|Heterotardigrada|Echiniscoidea   |Echiniscidae  |Echiniscus  |Echiniscus testudo       |NULL                |Echiniscus testudo (Doy�re, 1840)       |2253070 |2253070   |DK         |NULL    |NULL         |55.691167      |12.58065        |\n",
            "|910643299|2fce3ac5-5bd1-4c11-82c8-68bfb937399a|522                       |NULL           |2e7df380-8356-4533-bcb3-5459e23c794e|2013-05-17T00:00 |17 |5    |2013|HUMAN_OBSERVATION|NULL           |Animalia|Tardigrada|Eutardigrada    |Parachela       |Isohypsibiidae|Isohypsibius|Isohypsibius prosostomus |NULL                |Isohypsibius prosostomus Thulin, 1928   |2253666 |2253666   |DK         |NULL    |NULL         |55.691301      |12.581659       |\n",
            "|910643284|2fce3ac5-5bd1-4c11-82c8-68bfb937399a|508                       |NULL           |2e7df380-8356-4533-bcb3-5459e23c794e|2013-05-17T00:00 |17 |5    |2013|HUMAN_OBSERVATION|NULL           |Animalia|Tardigrada|Eutardigrada    |Parachela       |Hypsibiidae   |Hypsibius   |Hypsibius dujardini      |NULL                |Hypsibius dujardini (Doy�re, 1840)      |2253634 |2253634   |DK         |NULL    |NULL         |55.691375      |12.582216       |\n",
            "+---------+------------------------------------+--------------------------+---------------+------------------------------------+-----------------+---+-----+----+-----------------+---------------+--------+----------+----------------+----------------+--------------+------------+-------------------------+--------------------+----------------------------------------+--------+----------+-----------+--------+-------------+---------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Data Validation\n"
      ],
      "metadata": {
        "id": "09340GchfbcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This data validation step is to ensure that all data types match the expected schema\n",
        "and that the data is consistent and expected. The code above should complete schema validation as it will\n",
        "catch any errors with data that does not match the defined schema\"\"\"\n",
        "\n",
        "\n",
        "# Null Checks\n",
        "\n",
        "\n",
        "\"\"\" We don't want the primaryKey column: gbifID to ever be null, we also don't want the \"species\"\n",
        "and \"speciesKey\", and \"datasetKey\", to be null as we are given data labelled for Tardigrades, so we will check these columns.\n",
        "As well, we do not want \"countryCode\" to be null since it is the most general location data - missing this will take away\n",
        "a good chunk of important information.\"\"\"\n",
        "\n",
        "# tardigrade_null = tardigrade_df.filter(\"gbifID is null\" or \"species is null\" or [filter] or [filter] or [filter]).count()\n",
        "\n",
        "# The only column required to be unique is our primary key: gbifID which has already been checked\n",
        "\n"
      ],
      "metadata": {
        "id": "2ugDNAxFfd9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7a2abf-74c6-4d2d-ec2e-accdd9faa62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The occurrenceID column is not distinct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Tune Spark Configuration Parameters\n"
      ],
      "metadata": {
        "id": "qNxdaRDkg86H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark parameters are immuteable once a Spark Session is started, stop the Spark Session\n",
        "\n",
        "spark.stop()\n",
        "\n",
        "# Set up a new SparkSession with new config parameters\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GoogleColabSpark\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    #. ADD MORE CONFIG PARAMETERS HERE"
      ],
      "metadata": {
        "id": "RvlBqjoa4vCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Explore and play around with the new Data!"
      ],
      "metadata": {
        "id": "9aHTSA8T9VRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Take this space to create cool visualizations or ML models from the data we have\n",
        "processed above. The world is your oyester!\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XSnnM10p9aCF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}